{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5a344d-1a20-4154-ab4f-b299cafdacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from typing import Optional, Callable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.loops import train_loop, valid_loop, test_loop\n",
    "from scripts.dataset_template import DatasetTemplate\n",
    "from scripts.models.autoencoder_cnn import CNNAutoencoder\n",
    "from scripts.models.autoencoder_lin import LinearAutoencoder\n",
    "from scripts.models.autoencoder_lstm import LSTMAutoencoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1109e6df-7b3f-483c-b4f1-02677901ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "#check if cuda available\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80db0df6-2f54-49b9-9afb-f4699712f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f32ef8-9ff8-4929-81c0-fc1f009cf2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "path_data_main = \"Data_abs_only\"\n",
    "\n",
    "data_train_path = f\"/home/bolci/Documents/Projekty/5G_OPEN_RAN/Anomaly_detection/5G_Open_RAN/{path_data_main}/Data_channels/train/comeretial\"\n",
    "data_test_folder_path = f\"/home/bolci/Documents/Projekty/5G_OPEN_RAN/Anomaly_detection/5G_Open_RAN/{path_data_main}/Data_channels/test\"\n",
    "data_valid_folder_path = f\"/home/bolci/Documents/Projekty/5G_OPEN_RAN/Anomaly_detection/5G_Open_RAN/{path_data_main}/Data_channels/valid\"\n",
    "\n",
    "testing_folders = os.listdir(data_test_folder_path)\n",
    "valid_folders = os.listdir(data_valid_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2c5c5d-ab93-4d69-847c-fc1c7c862fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, values):\n",
    "    with open(file_name, \"w\") as output:\n",
    "        output.write(str(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aac9aa5-eeb0-45ca-9e1c-e4689e0f3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "batch_size = 32\n",
    "batch_size_test_valid = 1\n",
    "learning_rate = 0.001\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8be78a-0a63-4848-98e6-0f56065c6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms\n",
    "data_transforms = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7804ac-421f-430e-b9da-c9dbbf8341f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = '/home/bolci/Documents/Projekty/5G_OPEN_RAN/Anomaly_detection/5G_Open_RAN/scripts/Results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0cfc522-2144-44aa-956f-43135ad0a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.492267 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.338245  [    0/163420]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m   train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mloss_RMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------- TRUE BTS SCORE ----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m   valid_loss_true, valid_metric_true \u001b[38;5;241m=\u001b[39m valid_loop(valid_dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFake_Bts_PCI_466\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     77\u001b[0m                                                   model,\n\u001b[1;32m     78\u001b[0m                                                   loss_RMS,\n\u001b[1;32m     79\u001b[0m                                                   metric_fn_RMS)\n",
      "File \u001b[0;32m~/Documents/Projekty/5G_OPEN_RAN/Anomaly_detection/5G_Open_RAN/scripts/loops.py:10\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[1;32m      8\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   pred \u001b[38;5;241m=\u001b[39m model(X\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     13\u001b[0m   loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/Projekty/5G_OPEN_RAN/Anomaly_detection/5G_Open_RAN/scripts/dataset_template.py:29\u001b[0m, in \u001b[0;36mDatasetTemplate.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     28\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_names[idx])\n\u001b[0;32m---> 29\u001b[0m     loaded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     31\u001b[0m     loaded_data \u001b[38;5;241m=\u001b[39m loaded_data\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel]\u001b[38;5;241m*\u001b[39mloaded_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Projekty/5G_OPEN_RAN/Anomaly_detection/5G_Open_RAN/scripts/dataset_template.py:13\u001b[0m, in \u001b[0;36mDatasetTemplate.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDatasetTemplate\u001b[39;00m(Dataset):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     11\u001b[0m                  data_path: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m     12\u001b[0m                  label: Optional \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m---> 13\u001b[0m                  loader_f: Callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     14\u001b[0m                  transform: transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m                  label_to_one_hot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m data_path\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_names \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(data_path) \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/format.py:784\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    782\u001b[0m version \u001b[38;5;241m=\u001b[39m read_magic(fp)\n\u001b[1;32m    783\u001b[0m _check_version(version)\n\u001b[0;32m--> 784\u001b[0m shape, fortran_order, dtype \u001b[38;5;241m=\u001b[39m \u001b[43m_read_array_header\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    787\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/format.py:623\u001b[0m, in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version, max_header_size)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m# The header is a pretty-printed string representation of a literal\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# Python dictionary with trailing newlines padded to a ARRAY_ALIGN byte\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# boundary. The keys are strings.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# For performance reasons, we try without _filter_header first though\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43msafe_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/utils.py:1078\u001b[0m, in \u001b[0;36msafe_eval\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# Local import to speed up numpy's import time.\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ast.py:99\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mkeys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m     98\u001b[0m         _raise_malformed_node(node)\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, BinOp) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mop, (Add, Sub)):\n\u001b[1;32m    102\u001b[0m     left \u001b[38;5;241m=\u001b[39m _convert_signed_num(node\u001b[38;5;241m.\u001b[39mleft)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ast.py:88\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Tuple):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, List):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n",
      "File \u001b[0;32m/usr/lib/python3.10/ast.py:84\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_num(node)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert\u001b[39m(node):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant):\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#datasets and dataloaders\n",
    "\n",
    "\n",
    "b_s = [16, 32, 64, 128]\n",
    "learnin_rt = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "for batch_size in b_s:\n",
    "    for learning_rate in learnin_rt:\n",
    "\n",
    "        folder_name = F\"results_batch-size={batch_size}_learning-rate={learning_rate}\"\n",
    "        saving_path_with_folder = os.path.join(saving_path, folder_name)\n",
    "        \n",
    "        if not os.path.exists(saving_path_with_folder):\n",
    "            os.makedirs(saving_path_with_folder)\n",
    "\n",
    "        #train\n",
    "        train_dataset = DatasetTemplate(data_train_path, 0, transform = data_transforms)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        #valid\n",
    "        valid_datasets = {}\n",
    "        valid_dataloaders = {}\n",
    "        \n",
    "        for valid_folder_name in valid_folders:\n",
    "            valid_folder_path_all = os.path.join(data_valid_folder_path, valid_folder_name)\n",
    "            valid_datasets[valid_folder_name] = DatasetTemplate(valid_folder_path_all, 1)\n",
    "            valid_dataloaders[valid_folder_name] = DataLoader(valid_datasets[valid_folder_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        #test\n",
    "        test_datasets = {}\n",
    "        test_dataloaders = {}\n",
    "        \n",
    "        for test_folder_name in testing_folders:\n",
    "            test_folder_path_all = os.path.join(data_test_folder_path, test_folder_name)\n",
    "            test_datasets[test_folder_name] = DatasetTemplate(test_folder_path_all, 1)\n",
    "            test_dataloaders[test_folder_name] = DataLoader(test_datasets[test_folder_name], batch_size=batch_size_test_valid, shuffle=True)\n",
    "\n",
    "\n",
    "        #define model\n",
    "        model = CNNAutoencoder().to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        loss_RMS = lambda x,y: torch.sqrt(criterion(x, y))\n",
    "        metric_fn_RMS = lambda x,y: torch.sqrt(criterion(x, y))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        #for x,x in train_dataloader:\n",
    "        #    print(model(x.to('cuda')).shape)\n",
    "        \n",
    "        \n",
    "        valid_loop(test_dataloaders[list(test_dataloaders.keys())[0]],\n",
    "                  model,\n",
    "                  loss_RMS,\n",
    "                  metric_fn_RMS,\n",
    "                  device=device)\n",
    "\n",
    "\n",
    "        #epochs\n",
    "\n",
    "        # Lists to log the losses during the training process\n",
    "        train_losses = []\n",
    "        \n",
    "        valid_losses_true = []\n",
    "        valid_metrics_true = []\n",
    "        \n",
    "        valid_losses_false = []\n",
    "        valid_metrics_false = []\n",
    "        \n",
    "        for t in range(epochs):\n",
    "          print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "          train_loss = train_loop(train_dataloader,\n",
    "                                             model,\n",
    "                                             loss_RMS,\n",
    "                                             optimizer)\n",
    "        \n",
    "          print(\"------------- TRUE BTS SCORE ----------------\")\n",
    "          valid_loss_true, valid_metric_true = valid_loop(valid_dataloaders['Fake_Bts_PCI_466'],\n",
    "                                                          model,\n",
    "                                                          loss_RMS,\n",
    "                                                          metric_fn_RMS)\n",
    "        \n",
    "          print(\"------------- FALSE BTS SCORE ----------------\")\n",
    "          valid_loss_false, valid_metric_false = valid_loop(valid_dataloaders['Fake_Bts_PCI_466_wPA'],\n",
    "                                                            model,\n",
    "                                                            loss_RMS,\n",
    "                                                            metric_fn_RMS)\n",
    "            \n",
    "        \n",
    "          train_losses.append(train_loss)\n",
    "        \n",
    "          valid_losses_true.append(valid_loss_true)\n",
    "          valid_metrics_true.append(valid_metric_true)\n",
    "        \n",
    "          valid_losses_false.append(valid_loss_false)\n",
    "          valid_metrics_false.append(valid_metric_false)\n",
    "            \n",
    "        \n",
    "        print(\"Done!\")\n",
    "\n",
    "        model_save = 'model.pt'\n",
    "        saving_path_model = os.path.join(saving_path_with_folder, model_save)\n",
    "        torch.save(model.state_dict(), saving_path_model)\n",
    "\n",
    "        saving_path_figure = os.path.join(saving_path_with_folder, 'fig_1.png')\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses[10:], label=\"train\")\n",
    "        plt.plot(valid_losses_true[10:], label=\"losses True BTS\")\n",
    "        plt.plot(valid_losses_false[10:], label=\"losses False BTS\")\n",
    "        plt.legend()\n",
    "        plt.savefig(saving_path_figure)\n",
    "\n",
    "        saving_path_figure = os.path.join(saving_path_with_folder, 'fig_2.png')\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses, label=\"train\")\n",
    "        plt.plot(valid_losses_true, label=\"losses True BTS\")\n",
    "        plt.plot(valid_losses_false, label=\"losses False BTS\")\n",
    "        plt.legend()\n",
    "        plt.savefig(saving_path_figure)\n",
    "\n",
    "        \n",
    "        #saving scores\n",
    "        train_saving_path = os.path.join(saving_path_with_folder, 'train_scores.txt')\n",
    "        save_file(train_saving_path, train_losses)\n",
    "\n",
    "        validT_saving_path = os.path.join(saving_path_with_folder, 'valid_true_scores.txt')\n",
    "        save_file(validT_saving_path, valid_losses_true)\n",
    "\n",
    "        validF_saving_path = os.path.join(saving_path_with_folder, 'valid_false_scores.txt')\n",
    "        save_file(validF_saving_path, valid_losses_false)\n",
    "\n",
    "        \n",
    "        for testing_name, testing_loader in test_dataloaders.items():\n",
    "            test_losses, test_metrics = test_loop(testing_loader, \n",
    "                                                    model,\n",
    "                                                    loss_RMS,\n",
    "                                                     metric_fn_RMS)\n",
    "\n",
    "            #saving_scores\n",
    "            validF_saving_path = os.path.join(saving_path_with_folder, f'Ftest_{testing_name}_scores.txt')\n",
    "            save_file(validF_saving_path, test_losses)\n",
    "            \n",
    "            mean_loss = np.mean(np.array(test_losses))\n",
    "            mean_metrics = np.mean(np.array(test_metrics))\n",
    "            print(f\"Dataset name: {testing_name}, mean_loss: {mean_loss}, mean_acc: {mean_metrics}\")\n",
    "\n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f325d1-5724-424a-a98d-7e20cb1986aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in scores.items():\n",
    "    print(k)\n",
    "    print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cadd58-e825-4396-822f-cc3ada65312c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
